{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic Survival Prediction Analysis\n",
        "### Prepared for Non-Data Scientists\n",
        "\n",
        "This notebook performs a comprehensive analysis of the Titanic dataset to predict passenger survival. \n",
        "We use three industry-standard machine learning models: **Logistic Regression**, **Random Forest**, and **XGBoost**, \n",
        "along with an **Ensemble** that combines their strengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "We begin by loading our analytical tools. These include libraries for data handling (Pandas), \n",
        "mathematical operations (NumPy), and visual charts (Seaborn, Matplotlib)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# 1. Load all libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the Data\n",
        "We read the file provided in the local directory. The goal is to predict the \"Survived\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# 2. Load the dataset\n",
        "# Instruction: Ensure the file \"Titanic-Dataset.csv\" is in the same directory.\n",
        "try:\n",
        "    df = pd.read_csv(\"Titanic-Dataset.csv\")\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Titanic-Dataset.csv not found. Please ensure it is in the same directory.\")\n",
        "    # Fallback for demonstration if needed, but strictly following user instructions\n",
        "    # df = sns.load_dataset('titanic') \n",
        "\n",
        "# 3. Dataset exploration\n",
        "print(\"\\n--- Starting Exploratory Data Analysis ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n",
        "Before building a model, we must understand the \"DNA\" of our data. \n",
        "- We look at who survived based on their gender, class, and family size.\n",
        "- we check for missing information and outliers that might skew our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# a & d. Categorical value occurrence and relationship with target\n",
        "categorical_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "numerical_features = ['Age', 'Fare']\n",
        "target = 'Survived'\n",
        "\n",
        "for col in categorical_features:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.countplot(data=df, x=col, palette='viridis')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.barplot(data=df, x=col, y=target, palette='magma')\n",
        "    plt.title(f'Survival Rate by {col}')\n",
        "    plt.show()\n",
        "\n",
        "# b. Correlation between categorical and numerical\n",
        "# We look at distributions of Age/Fare across categories\n",
        "for num_col in numerical_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.boxplot(data=df, x='Pclass', y=num_col, hue='Survived')\n",
        "    plt.title(f'{num_col} distribution by Pclass and Survival')\n",
        "    plt.show()\n",
        "\n",
        "# c. Missing values analysis\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"\\nMissing Values:\\n\", missing_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Value Map (Yellow indicates missing)')\n",
        "plt.show()\n",
        "\n",
        "# Investigating if missingness is random (e.g., Age missingness vs Pclass)\n",
        "df['Age_Missing'] = df['Age'].isnull().astype(int)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df, x='Pclass', y='Age_Missing')\n",
        "plt.title('Propensity of Missing Age by Pclass')\n",
        "plt.show()\n",
        "\n",
        "# e. Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# f. Focus on high correlation to Target\n",
        "target_corr = corr_matrix[target].sort_values(ascending=False)\n",
        "print(\"\\nCorrelation with Survived:\\n\", target_corr)\n",
        "\n",
        "# g. Distribution and Outliers\n",
        "for col in numerical_features:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df[col].dropna(), kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x=df[col].dropna())\n",
        "    plt.title(f'Outliers in {col}')\n",
        "    plt.show()\n",
        "\n",
        "# h. Write up findings (Printed to console as summary)\n",
        "print(\"\"\"\n",
        "--- EDA FINDINGS SUMMARY ---\n",
        "1. Pclass and Sex are strongest predictors. Females and 1st Class passengers have significantly higher AUC potential.\n",
        "2. Age has ~20% missing values, correlated with Pclass, suggesting MNAR (Missing Not At Random) or MAR depending on subgroup.\n",
        "3. Fare is highly skewed with significant outliers; log transformation or robust scaling is recommended.\n",
        "4. SibSp and Parch suggest that traveling alone or with very large families reduces survival chance.\n",
        "\"\"\")\n",
        "\n",
        "# 4. Data engineering\n",
        "print(\"\\n--- Data Engineering ---\")\n",
        "# Transform features to protect from outliers\n",
        "df['Fare_Log'] = np.log1p(df['Fare'])\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
        "\n",
        "# 5. Preprocessing\n",
        "# a. Split 60/20/20\n",
        "X = df.drop(['Survived', 'Age_Missing', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# First split: 60% train, 40% temp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering & Preprocessing\n",
        "We transform raw data into \"features\" that the computer can understand. \n",
        "- **MICE Imputation**: A sophisticated method to \"guess\" missing ages based on other clues like ticket price and class.\n",
        "- **SMOTE**: Since we have fewer survivors than non-survivors, we use this technique to balance the dataset so the model doesn't become biased.\n",
        "- **Splitting**: We split the data into 60% Training (to learn), 20% Validation (to practice), and 20% Test (the final exam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "# Second split: 20% test, 20% validation (which is 50% of the 40% temp)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# b. Preprocessing Pipeline\n",
        "num_cols = ['Age', 'Fare_Log', 'FamilySize']\n",
        "cat_cols = ['Pclass', 'Sex', 'Embarked', 'IsAlone']\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(random_state=42)), # MICE Imputation\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols)\n",
        "    ])\n",
        "\n",
        "# 6. Fine tune models\n",
        "print(\"\\n--- Training & Hyperparameter Tuning ---\")\n",
        "\n",
        "def tune_model(clf, params, name):\n",
        "    # SMOTE is used inside ImbPipeline to avoid data leakage\n",
        "    # RFE is added before the classifier to select best features\n",
        "    pipeline = ImbPipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('rfe', RFE(estimator=clf)),\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    \n",
        "    # Add RFE parameters to the grid search\n",
        "    # We iterate through different numbers of features to select\n",
        "    params['rfe__n_features_to_select'] = [5, 8, 10]\n",
        "    \n",
        "    grid = GridSearchCV(pipeline, params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    print(f\"{name} Best AUC: {grid.best_score_:.4f}\")\n",
        "    return grid.best_estimator_\n",
        "\n",
        "# Logistic Regression with class_weight='balanced'\n",
        "lr_params = {'classifier__C': [0.01, 0.1, 1, 10]}\n",
        "best_lr = tune_model(LogisticRegression(class_weight='balanced'), lr_params, \"Logistic Regression\")\n",
        "\n",
        "# Random Forest with class_weight='balanced'\n",
        "rf_params = {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [None, 10, 20]}\n",
        "best_rf = tune_model(RandomForestClassifier(class_weight='balanced', random_state=42), rf_params, \"Random Forest\")\n",
        "\n",
        "# XGBoost (scale_pos_weight used for imbalance)\n",
        "ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "xgb_params = {'classifier__learning_rate': [0.01, 0.1], 'classifier__n_estimators': [100, 200]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training & Tuning\n",
        "We \"fine-tune\" each model, much like adjusting a radio dial, to find the settings that produce the best **AUC** (Area Under the Curve).\n",
        "An AUC of 1.0 is perfect; 0.5 is no better than a coin flip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_xgb = tune_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n",
        "\n",
        "# b. Dynamic Ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('lr', best_lr), ('rf', best_rf), ('xgb', best_xgb)],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble.fit(X_train, y_train)\n",
        "print(\"Ensemble model trained.\")\n",
        "\n",
        "# 7. Model Evaluation\n",
        "print(\"\\n--- Model Evaluation (Validation Set) ---\")\n",
        "models = {\n",
        "    'Logistic Regression': best_lr,\n",
        "    'Random Forest': best_rf,\n",
        "    'XGBoost': best_xgb,\n",
        "    'Ensemble': ensemble\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, model in models.items():\n",
        "    y_prob = model.predict_proba(X_val)[:, 1]\n",
        "    auc = roc_auc_score(y_val, y_prob)\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})')\n",
        "    print(f\"{name} Validation AUC: {auc:.4f}\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves - Validation Set')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 8. SHAP Interpretability\n",
        "print(\"\\n--- SHAP Interpretability ---\")\n",
        "# Pre-transform data for SHAP (using the preprocessor from the pipeline)\n",
        "# We use the preprocessor from one of the best models (they all use the same one)\n",
        "preprocessor_fitted = best_rf.named_steps['preprocessor']\n",
        "X_train_transformed = preprocessor_fitted.transform(X_train)\n",
        "X_val_transformed = preprocessor_fitted.transform(X_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation (AUC & ROC)\n",
        "The **ROC Curve** helps us visualize how well our model balances catching survivors without incorrectly labeling non-survivors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Get feature names after one-hot encoding\n",
        "cat_features_transformed = preprocessor_fitted.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\n",
        "all_feature_names = np.concatenate([num_cols, cat_features_transformed])\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nProcessing SHAP for {name}...\")\n",
        "    # Get the actual classifier part of the pipeline\n",
        "    if name == 'Ensemble':\n",
        "        # Voting classifier doesn't easily support single SHAP explainer, skipping or using first estimator\n",
        "        print(\"Skipping detailed SHAP for Ensemble for brevity, using XGB component summary.\")\n",
        "        clf = ensemble.named_estimators_['xgb'].named_steps['classifier']\n",
        "    else:\n",
        "        clf = model.named_steps['classifier']\n",
        "    \n",
        "    try:\n",
        "        explainer = shap.Explainer(clf, X_train_transformed)\n",
        "        shap_values = explainer(X_val_transformed)\n",
        "        \n",
        "        plt.figure()\n",
        "        plt.title(f'SHAP Summary Plot: {name}')\n",
        "        shap.summary_plot(shap_values, X_val_transformed, feature_names=all_feature_names, show=False)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP error for {name}: {e}\")\n",
        "\n",
        "# 9. False Positives Analysis & Next Steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Interpretability (SHAP)\n",
        "We use **SHAP** values to explain *why* the model made its decisions. It shows us which features (like being female or in 1st class) \"pushed\" the prediction toward survival."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n--- Error Analysis & Next Steps ---\")\n",
        "y_val_pred = ensemble.predict(X_val)\n",
        "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Ensemble Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "NEXT STEPS TO IMPROVE MODELS:\n",
        "1. Feature Interaction: Explore interactions like 'Sex' x 'Pclass'.\n",
        "2. Advanced Imputation: Further tune the IterativeImputer (MICE) parameters.\n",
        "3. Feature Selection: Use Recursive Feature Elimination to remove noise.\n",
        "4. Deeper Tuning: Expand XGBoost hyperparameter search (gamma, min_child_weight).\n",
        "5. Error Analysis: Investigate specific false positives (likely high-class males who perished despite the 'Women and Children first' rule).\n",
        "\"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Analysis & Next Steps\n",
        "We look at where we failed (False Positives) and provide a roadmap for even higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}